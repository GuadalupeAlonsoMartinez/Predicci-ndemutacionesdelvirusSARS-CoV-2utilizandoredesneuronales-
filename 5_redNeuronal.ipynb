{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-gkowr9z8h-"
      },
      "source": [
        "**Montar colab con drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nREnj61GjM1-"
      },
      "outputs": [],
      "source": [
        "#para montar mi drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgACbSy10IAT"
      },
      "source": [
        "**Librerias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XVADSqvjIq0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9NjP95bFVEy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "#carpetaOrigen = '/content/gdrive/MyDrive/validacion/x_validacionImg/'\n",
        "\n",
        "def cargarSetImagenes(carpetaOrigen = ''):\n",
        "  contenido = os.listdir(carpetaOrigen)\n",
        "  imagenes = []\n",
        "  for fichero in contenido:\n",
        "      img = cv2.imread(carpetaOrigen + fichero,cv2.IMREAD_GRAYSCALE)\n",
        "      imagenes.append(img)\n",
        "  return np.array(imagenes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVt73JH8DD4N"
      },
      "outputs": [],
      "source": [
        "train_data = cargarSetImagenes(\"/content/gdrive/MyDrive/IA-SARS/archivosBinarioSeparado/entrenamiento/x_entrenamientoImg/\")\n",
        "train_dataY = cargarSetImagenes(\"/content/gdrive/MyDrive/IA-SARS/archivosBinarioSeparado/entrenamiento/y_entrenamientoImg/\")\n",
        "test_data = cargarSetImagenes(\"/content/gdrive/MyDrive/IA-SARS/archivosBinarioSeparado/validacion/x_validacionImg/\")\n",
        "test_dataY = cargarSetImagenes(\"/content/gdrive/MyDrive/IA-SARS/archivosBinarioSeparado/validacion/y_validacionImg/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbP43ATa0KKm"
      },
      "outputs": [],
      "source": [
        "print(train_data)\n",
        "print(len(train_dataY))\n",
        "print(len(test_data))\n",
        "print(len(test_dataY))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeftEVa9KEcO"
      },
      "outputs": [],
      "source": [
        "print(test_data[0].shape) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0R-lNZq1bF1"
      },
      "source": [
        "**Normalizacion de datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9iElMIRjLpt"
      },
      "outputs": [],
      "source": [
        "###Etapa de preprocesamiento de datos\n",
        "def preprocesamiento(array): # mandar a llamar mis imagenes de entrenamiento x y y\n",
        "# Normalizamos (valores entres 0 y 1) las imagenes y los casteamos al formato correcto, escala de grises. \n",
        "    arrayNormalizado = []\n",
        "    for matrizImg in array:\n",
        "      arrayImg = matrizImg.astype(\"float32\") / 255.0\n",
        "      #print(\"Se hizo la division\");\n",
        "      #print(\"Size: \"+str(len(arrayImg))+'x'+str(len(arrayImg[0])) )\n",
        "      arrayImg = np.reshape(arrayImg[:,:], (172, 172, 1))#Imágenes de 172x172px con un solo canal(escala de grises)\n",
        "      arrayNormalizado.append(arrayImg)\n",
        "    return np.array(arrayNormalizado)#Retornamos el arreglo de datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAPp0esg0pGE"
      },
      "source": [
        "***Cargar los datos entrenamiento y validacion ***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoC2bMZ_kNcd"
      },
      "outputs": [],
      "source": [
        "#Cargamos los datos, en este caos como usaremos el dataset para codificar y decodificar, de imagen a imgagen, no es necesario cargar las etiquetas\n",
        "###(train_data, _), (test_data, _) = mnist.load_data()#* cargar x entrenamiento  validacion x \n",
        "\n",
        "# Normalizamos y ajustamos la imagenes al formato correcto**}----\n",
        "train_data = preprocesamiento(train_data)#Prepocesamos el conjunto de entrenamiento\n",
        "train_dataY = preprocesamiento(train_dataY)\n",
        "test_data = preprocesamiento(test_data)#Prepocesamos el conjunto de pruebas\n",
        "test_dataY = preprocesamiento(test_dataY)\n",
        "\n",
        "print(train_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZHzYe5Y1F0z"
      },
      "source": [
        "**creacion del modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24TXkBFVsWn_"
      },
      "outputs": [],
      "source": [
        "##Vamos a crear el modelo agregando elementos al objeto layers\n",
        "##La primera configuración es el tamaño de la capa de entrada, es el tamaño de las imágenes (28x28x1)\n",
        "input = layers.Input(shape=(172, 172, 1))\n",
        "\n",
        "# Encoder(Codificador)\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "\n",
        "# Decoder(Decodificador)\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)#La funcion Conv2DTranspose se encarga de aumentar el tamaño de la matriz de salida\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder = Model(input, x)\n",
        "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "autoencoder.summary()#resume la arquitectura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm_XlOmY1Ljj"
      },
      "source": [
        "**Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwtxFs0WuUIB"
      },
      "outputs": [],
      "source": [
        "#Vamos a entrenar el modelo usando como entrada imagenes con ruido, esperando que a la salida nos entregue una imagen limpia.\n",
        "#En este caso estamos usando dos conjuntos de imagenes, las que tienen ruido y las que estan limpias(las esperadas)\n",
        "\n",
        "autoencoder.fit(\n",
        "    x= train_data,#Conjunto de entrenamiento con ruido\n",
        "    y=train_dataY, #Conjunto de entrenamiento sin ruido (estas son las que esperamos que entregue el modelo)\n",
        "    epochs=100,\n",
        "    batch_size=128, \n",
        "    shuffle = True,\n",
        "    verbose=1,\n",
        "    validation_data=(test_data, test_dataY),#La validación se hará con el conjunto con ruido comparado con las esperadas\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPCzlRtj1QAL"
      },
      "source": [
        "**Pruebas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-PiJU8AysCK"
      },
      "outputs": [],
      "source": [
        "#Primero vamos a probar el modelo con las imágenes de prueba sin agregar ruido para ver como la recupera\n",
        "predictions = autoencoder.predict(test_data)\n",
        "#display(predictions, test_data )#Imprimimos el resultado de algunas imagenes usadas con su predicción (arriba=imagen de entrada, abajo=imagen resultado del modelo)\n",
        "#Ahora el modelo es capaz de limpiar las imágenes, obteniendo una imagen diferente a la de entrada"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Agregadas mi\n",
        "from numpy import save\n",
        "save('/content/gdrive/MyDrive/IA-SARS/resultados_entrenamiento/predictions20.npy', predictions)\n",
        "#Siempre llegar hasta aqui"
      ],
      "metadata": {
        "id": "a6tX1GwuCyjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agregadas mi\n",
        "# load numpy array from npy file\n",
        "from numpy import load\n",
        "# load array\n",
        "predictions = load('/content/gdrive/MyDrive/IA-SARS/resultados_entrenamiento/predictions20.npy')\n"
      ],
      "metadata": {
        "id": "scOgRp_rD66Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataNN = predictions.astype(\"float32\") * 255.0 # normalizamos la matriz de predicciones\n"
      ],
      "metadata": {
        "id": "WRJwpoIgJ9fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance # importo el objeto que me permitara usar la funcion de distancia\n",
        "\n",
        "test_dataNN = predictions.astype(\"float32\") * 255.0 # normalizamos la matriz de predicciones\n",
        "#print(test_dataNN[0])\n",
        "\n",
        "def fun(value):#Definimos la funcion que se aplicara a cada elemento de la matris\n",
        "    distances = []#Se dedine el arreglo donde se guardaran las distancias\n",
        "    distances.append(distance.euclidean((value,0),(0,0)))#0 Se calcula la distancia del valor respecto a 0\n",
        "    distances.append(distance.euclidean((value,0),(64,0)))#1 Se calcula la distancia del valor respecto a 64\n",
        "    distances.append(distance.euclidean((value,0),(128,0)))#2 Se calcula la distancia del valor respecto a 128\n",
        "    distances.append(distance.euclidean((value,0),(255,0)))#3 Se calcula la distancia del valor respecto a 255\n",
        "    distanceMin = min(distances) # Se obtiene la minima distancia del arreglo de distancias\n",
        "    indexMin = distances.index(distanceMin) #En base a la minima distancia se obtiene el indice correspondiente para saber a cual esta mas cerca\n",
        "    valueParse = 0 #Se define la variable donde se retornara el valor que corresponde a la distancia mas corta euclaniana\n",
        "    if indexMin == 0: # Si el indice es 0 se entiende que esta mas cerca a 0\n",
        "      valueParse = 0\n",
        "    elif indexMin == 1:# Si el indice es 1 se entiende que esta mas cerca a 64\n",
        "      valueParse = 64\n",
        "    elif indexMin == 2: # Si el indice es 2 se entiende que esta mas cerca a 128\n",
        "      valueParse = 128\n",
        "    elif indexMin == 3:# Si el indice es 3 se entiende que esta mas cerca a 255\n",
        "      valueParse = 255\n",
        "\n",
        "    return valueParse # Se retorna el valor a la aproximacion correspondiente\n",
        "\n",
        "vfunc = np.vectorize(fun)#  coonvierte la funcion a una F. Numpy\n",
        "result = vfunc(test_dataNN) #Se implementa la funcion\n",
        "\n",
        "#print(test_dataNN)\n",
        "\n",
        "print(result)#Se implimen los resultados"
      ],
      "metadata": {
        "id": "5-iXRbx1pG8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxlovr3XpHWX"
      },
      "outputs": [],
      "source": [
        "test_dataNN = test_data.astype(\"float32\") * 255.0\n",
        "\n",
        "def compareSecuenses(secuencePrediction,secuenceTest):\n",
        "  coinciden = 0\n",
        "  secPre = np.asarray(secuencePrediction).reshape(-1)\n",
        "  secTes = np.asarray(secuenceTest).reshape(-1)\n",
        "  for index in range(0,len(secPre)-181):\n",
        "    #print(secPre[index],'-',secTes[index])\n",
        "    if secPre[index] == secTes[index]:\n",
        "      coinciden = coinciden + 1\n",
        "\n",
        "  return coinciden\n",
        "\n",
        "\n",
        "totalCoincidents = []\n",
        "for index in range(0,len(test_dataNN)):\n",
        "  totalCoincidents.append(compareSecuenses(result[index],test_dataNN[index]))\n",
        "  \n",
        "print('promedio: ',sum(totalCoincidents)/29403) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(totalCoincidents)/29403) "
      ],
      "metadata": {
        "id": "960V7OyBTekC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}